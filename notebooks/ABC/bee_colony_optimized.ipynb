{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T23:13:16.413323Z",
     "iopub.status.busy": "2024-12-07T23:13:16.412474Z",
     "iopub.status.idle": "2024-12-07T23:13:18.340689Z",
     "shell.execute_reply": "2024-12-07T23:13:18.339963Z",
     "shell.execute_reply.started": "2024-12-07T23:13:16.413275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = \"../../dataset/old_complete\"\n",
    "directory = os.fsencode(dataset_path)\n",
    "file_list = os.listdir(directory)\n",
    "dfs = []\n",
    "for file in file_list:\n",
    "    filename = os.fsdecode(file)\n",
    "    dfs.append(pd.read_csv(f\"{dataset_path}/{filename}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T23:13:20.670296Z",
     "iopub.status.busy": "2024-12-07T23:13:20.669965Z",
     "iopub.status.idle": "2024-12-07T23:13:20.828098Z",
     "shell.execute_reply": "2024-12-07T23:13:20.827247Z",
     "shell.execute_reply.started": "2024-12-07T23:13:20.670267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    nan_number = dfs[i].isna().sum().sum()\n",
    "    if nan_number > 0:\n",
    "        print(f\"We have {nan_number} nan values in df {i}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T23:13:22.299064Z",
     "iopub.status.busy": "2024-12-07T23:13:22.298729Z",
     "iopub.status.idle": "2024-12-07T23:13:22.317537Z",
     "shell.execute_reply": "2024-12-07T23:13:22.316617Z",
     "shell.execute_reply.started": "2024-12-07T23:13:22.299035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dfs[6].dropna(inplace=True)\n",
    "dfs[18].dropna(inplace=True)\n",
    "dfs[33].dropna(inplace=True)\n",
    "dfs[96].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T08:53:15.106195Z",
     "iopub.status.busy": "2024-12-08T08:53:15.105744Z",
     "iopub.status.idle": "2024-12-08T08:53:15.111087Z",
     "shell.execute_reply": "2024-12-08T08:53:15.110030Z",
     "shell.execute_reply.started": "2024-12-08T08:53:15.106163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CORR_COLS = ['CD',\n",
    " 'NII',\n",
    " 'NL',\n",
    " 'NLE',\n",
    " 'TCD',\n",
    " 'WarningBlocker',\n",
    " 'WarningCritical',\n",
    " 'WarningMajor',\n",
    " 'WarningMinor',\n",
    " 'Documentation Metric Rules',\n",
    " 'HTRP',\n",
    " 'TNOS',\n",
    " 'HPL',\n",
    " 'HEFF',\n",
    " 'Anti Pattern',\n",
    " 'TLLOC',\n",
    " 'McCC',\n",
    " 'HDIF',\n",
    " 'HPV',\n",
    " 'TLOC',\n",
    " 'LOC',\n",
    " 'Size Metric Rules',\n",
    " 'HVOL',\n",
    " 'DLOC',\n",
    " 'TCLOC',\n",
    " 'HCPL',\n",
    " 'WarningInfo',\n",
    " 'NOS',\n",
    " 'HNDB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T23:13:54.685705Z",
     "iopub.status.busy": "2024-12-07T23:13:54.685004Z",
     "iopub.status.idle": "2024-12-07T23:13:55.060654Z",
     "shell.execute_reply": "2024-12-07T23:13:55.059865Z",
     "shell.execute_reply.started": "2024-12-07T23:13:54.685670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def df_transformation(df: pd.DataFrame, correlated_cols) -> pd.DataFrame:\n",
    "    df.drop([\"Unnamed: 0\", \"Name\", \"LongName\", \"Parent\", \"Component\", \"Path\", \"Line\", \"Column\", \"EndLine\", \"EndColumn\", \"ID\"],\n",
    "            axis=1, inplace=True)\n",
    "    df.drop(correlated_cols, axis=1, inplace=True)\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    return df_scaled\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    dfs[i] = df_transformation(dfs[i], CORR_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T23:13:57.223122Z",
     "iopub.status.busy": "2024-12-07T23:13:57.222628Z",
     "iopub.status.idle": "2024-12-07T23:13:57.242701Z",
     "shell.execute_reply": "2024-12-07T23:13:57.241732Z",
     "shell.execute_reply.started": "2024-12-07T23:13:57.223084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T23:14:09.778049Z",
     "iopub.status.busy": "2024-12-07T23:14:09.777703Z",
     "iopub.status.idle": "2024-12-07T23:14:09.783696Z",
     "shell.execute_reply": "2024-12-07T23:14:09.782827Z",
     "shell.execute_reply.started": "2024-12-07T23:14:09.778017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dfs[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T20:17:41.047565Z",
     "iopub.status.busy": "2024-12-06T20:17:41.046764Z",
     "iopub.status.idle": "2024-12-06T20:17:41.054647Z",
     "shell.execute_reply": "2024-12-06T20:17:41.053656Z",
     "shell.execute_reply.started": "2024-12-06T20:17:41.047532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_df(df: pd.DataFrame):\n",
    "    new_dfs = []\n",
    "    if len(df) > 6000:\n",
    "        new_dfs.extend(split_df(df.iloc[:int(len(df)/2), :]))\n",
    "        new_dfs.extend(split_df(df.iloc[int(len(df)/2):, :]))\n",
    "    else:\n",
    "        new_dfs.append(df)\n",
    "    return new_dfs\n",
    "\n",
    "short_dfs = []\n",
    "df_names = []\n",
    "for i in range(len(dfs)):\n",
    "    new_dfs = split_df(dfs[i])\n",
    "    new_names = [file_list[i]] if len(new_dfs) <= 1 else [f\"{file_list[i]}_{j+1}\" for j in range(len(new_dfs))]\n",
    "    short_dfs.extend(new_dfs)\n",
    "    df_names.extend(new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T20:17:44.198384Z",
     "iopub.status.busy": "2024-12-06T20:17:44.197479Z",
     "iopub.status.idle": "2024-12-06T20:17:44.205806Z",
     "shell.execute_reply": "2024-12-06T20:17:44.204747Z",
     "shell.execute_reply.started": "2024-12-06T20:17:44.198334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_population(num_food_sources, num_features, feature_count):\n",
    "    rng = np.random.default_rng()\n",
    "    population = np.zeros((num_food_sources, num_features), dtype=int)\n",
    "\n",
    "    for i in range(num_food_sources):\n",
    "        # randomly select feature_count indices to set to 1\n",
    "        active_indices = rng.choice(num_features, size=feature_count, replace=False)\n",
    "        population[i, active_indices] = 1\n",
    "    return population\n",
    "\n",
    "# used for generation of solutions during the mutation process\n",
    "def jaccard_dissimilarity(X1, X2):\n",
    "    intersection = np.sum(np.logical_and(X1, X2))\n",
    "    union = np.sum(np.logical_or(X1, X2))\n",
    "    return 1 - intersection / union if union > 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T20:17:46.725329Z",
     "iopub.status.busy": "2024-12-06T20:17:46.724659Z",
     "iopub.status.idle": "2024-12-06T20:17:46.735088Z",
     "shell.execute_reply": "2024-12-06T20:17:46.734159Z",
     "shell.execute_reply.started": "2024-12-06T20:17:46.725297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def differential_mutation(food_sources, feature_count, phi, rng):\n",
    "    num_sources = len(food_sources)\n",
    "    mutant_sources = np.zeros_like(food_sources)\n",
    "\n",
    "    for i in range(num_sources):\n",
    "        # choose 3 random neighbours\n",
    "        r1, r2, r3 =  rng.choice([j for j in range(num_sources) if j != i], size=3, replace=False)\n",
    "        Xr1, Xr2, Xr3 = food_sources[r1], food_sources[r2], food_sources[r3]\n",
    "\n",
    "        # calculate scaled jackard dissimilarity\n",
    "        dissimilarity_r2_r3 = jaccard_dissimilarity(Xr2, Xr3)\n",
    "        target_dissimilarity = phi * dissimilarity_r2_r3\n",
    "        \n",
    "        # estimate similarity between new solution and r1\n",
    "        m1 = np.sum(Xr1)\n",
    "        m0 = len(Xr1) - m1\n",
    "\n",
    "        # estimate optimal ms\n",
    "        best_M11, best_M10, best_M01 = 0, 0, 0\n",
    "        min_difference = float('inf')\n",
    "\n",
    "        for M11 in range(m1 + 1):\n",
    "            for M10 in range(m0+1):\n",
    "                M01 = m1 - M11\n",
    "                denominator = M11 + M10 + M01\n",
    "                dissimilarity = 1 - (M11 / denominator if denominator != 0 else 1)\n",
    "                difference = abs(dissimilarity - target_dissimilarity)\n",
    "\n",
    "                if difference < min_difference:\n",
    "                    best_M11, best_M10, best_M01 = M11, M10, M01\n",
    "                    min_difference = difference\n",
    "\n",
    "        # compose le mutant\n",
    "        omega_i = np.zeros(len(food_sources[i]), dtype=int)\n",
    "\n",
    "        active_indices = np.where(Xr1 == 1)[0]\n",
    "        if len(active_indices) >= best_M11:\n",
    "            selected_indices = rng.choice(active_indices, size=best_M11, replace=False)\n",
    "            omega_i[selected_indices] = 1\n",
    "\n",
    "        inactive_indices = np.where(Xr1 == 0)[0]\n",
    "        if len(inactive_indices) >= best_M10:\n",
    "            selected_indices = rng.choice(inactive_indices, size=best_M10, replace=False)\n",
    "            omega_i[selected_indices] = 1\n",
    "\n",
    "        current_active_count = np.sum(omega_i)\n",
    "\n",
    "        if current_active_count < feature_count:\n",
    "            remaining_inactive_indices = np.where(omega_i == 0)[0]\n",
    "            additional_indices = rng.choice(remaining_inactive_indices, size=feature_count - current_active_count, replace=False)\n",
    "            omega_i[additional_indices] = 1\n",
    "\n",
    "        elif current_active_count > feature_count:\n",
    "            excess_active_indices = np.where(omega_i == 1)[0]\n",
    "            removal_indices = rng.choice(excess_active_indices, size=current_active_count - feature_count, replace=False)\n",
    "            omega_i[removal_indices] = 0\n",
    "\n",
    "        mutant_sources[i] = omega_i\n",
    "        # print(mutant_sources)\n",
    "\n",
    "    return mutant_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T20:17:49.256580Z",
     "iopub.status.busy": "2024-12-06T20:17:49.255889Z",
     "iopub.status.idle": "2024-12-06T20:17:49.261883Z",
     "shell.execute_reply": "2024-12-06T20:17:49.260997Z",
     "shell.execute_reply.started": "2024-12-06T20:17:49.256547Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def crossover(parent, mutant, crossover_rate, rng, num_features):\n",
    "    offspring = np.where(rng.random(len(parent)) < crossover_rate, mutant, parent)\n",
    "    current_active_count = np.sum(offspring)\n",
    "    if current_active_count < num_features:\n",
    "        inactive_indices = np.where(offspring == 0)[0]\n",
    "        additional_indices = rng.choice(inactive_indices, size=num_features - current_active_count, replace=False)\n",
    "        offspring[additional_indices] = 1\n",
    "\n",
    "    elif current_active_count > num_features:\n",
    "        active_indices = np.where(offspring == 1)[0]\n",
    "        removal_indices = rng.choice(active_indices, size=current_active_count - num_features, replace=False)\n",
    "        offspring[removal_indices] = 0\n",
    "\n",
    "    # print(offspring)\n",
    "    return offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T20:17:50.822966Z",
     "iopub.status.busy": "2024-12-06T20:17:50.822308Z",
     "iopub.status.idle": "2024-12-06T20:17:51.380331Z",
     "shell.execute_reply": "2024-12-06T20:17:51.379430Z",
     "shell.execute_reply.started": "2024-12-06T20:17:50.822934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import cupy as cp\n",
    "\n",
    "def optimized_sammon_error(high_distances, low_dim_data):\n",
    "    low_distances = cp.linalg.norm(low_dim_data[:, None] - low_dim_data, axis=2)\n",
    "    high_dist_sum = cp.sum(high_distances)\n",
    "    sammon_error_value = cp.sum(((high_distances - low_distances) ** 2) / (high_distances + 1e-9)) / high_dist_sum\n",
    "    return sammon_error_value\n",
    "\n",
    "def reduce_features(data, binary_vector):\n",
    "    selected_features = data[:, binary_vector == 1]\n",
    "    return selected_features\n",
    "\n",
    "def mdisabc(num_food_sources, crossover_rate, phi, MAX_LIMIT, max_iterations, feature_count, num_features, dataset: pd.DataFrame):\n",
    "    rng = np.random.default_rng()\n",
    "    food_sources = initialize_population(num_food_sources, num_features, feature_count)\n",
    "    limits = np.zeros(num_food_sources)\n",
    "    best_solution = None\n",
    "    best_error = float('inf')\n",
    "    error_history = []\n",
    "    dataset_norms = cp.linalg.norm(dataset[:, None] - dataset, axis=2)\n",
    "    low_dim_errors = np.zeros(num_food_sources)\n",
    "    sammon_errors = {}\n",
    "\n",
    "    for iteration in tqdm(range(max_iterations)):\n",
    "        mutants = differential_mutation(food_sources, feature_count, phi, rng)\n",
    "        for i in range(num_food_sources):\n",
    "            if tuple(food_sources[i]) not in sammon_errors:\n",
    "                subset_data = reduce_features(dataset, food_sources[i])\n",
    "                current_error = optimized_sammon_error(dataset_norms, subset_data)\n",
    "                sammon_errors[tuple(food_sources[i])] = current_error\n",
    "                # print(current_error)\n",
    "            else:\n",
    "                current_error = sammon_errors[tuple(food_sources[i])]\n",
    "\n",
    "            mutant = mutants[i]\n",
    "            candidate_solution = crossover(food_sources[i], mutant, crossover_rate, rng, feature_count)\n",
    "\n",
    "            if tuple(candidate_solution) not in sammon_errors:\n",
    "                neighbor_subset_data = reduce_features(dataset, candidate_solution)\n",
    "                neighbor_error = optimized_sammon_error(dataset_norms, neighbor_subset_data)\n",
    "                sammon_errors[tuple(candidate_solution)] = neighbor_error\n",
    "            else:\n",
    "                neighbor_error = sammon_errors[tuple(candidate_solution)]\n",
    "\n",
    "            if neighbor_error < current_error:\n",
    "                food_sources[i] = candidate_solution\n",
    "                low_dim_errors[i] = neighbor_error\n",
    "                limits[i] = 0\n",
    "            else:\n",
    "                limits[i] += 1\n",
    "            # print(food_sources[i])\n",
    "\n",
    "            if current_error < best_error:\n",
    "                best_solution = food_sources[i]\n",
    "                best_error = current_error\n",
    "\n",
    "        for i in range(num_food_sources):\n",
    "            if limits[i] >= MAX_LIMIT:\n",
    "                pos_indices = rng.choice(num_features, size=feature_count, replace=False)\n",
    "                food_sources[i]=np.array([1 if i in pos_indices else 0 for i in range(num_features)])\n",
    "                limits[i] = 0\n",
    "\n",
    "        error_history.append(best_error)\n",
    "\n",
    "    return best_solution, error_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T20:17:56.982084Z",
     "iopub.status.busy": "2024-12-06T20:17:56.981772Z",
     "iopub.status.idle": "2024-12-06T20:17:56.986746Z",
     "shell.execute_reply": "2024-12-06T20:17:56.985760Z",
     "shell.execute_reply.started": "2024-12-06T20:17:56.982057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "open(\"abc.log\", \"a\").close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T23:18:26.213602Z",
     "iopub.status.busy": "2024-11-14T23:18:26.212926Z",
     "iopub.status.idle": "2024-11-14T23:18:33.441938Z",
     "shell.execute_reply": "2024-11-14T23:18:33.440591Z",
     "shell.execute_reply.started": "2024-11-14T23:18:26.213564Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "with open(\"abc.log\", \"a\", buffering=1) as f:\n",
    "    for i in range(0, len(short_dfs)):\n",
    "        print(f\"Working on {df_names[i]}...\\n\\n\")\n",
    "        f.write(f\"Working on {df_names[i]}...\\n\\n\")\n",
    "        for feature_count in range(2, len(short_dfs[i].columns)-1):\n",
    "            best_solution, error_history = mdisabc(30, 0.25, 0.9, 50, 50, feature_count, \n",
    "                                                   len(short_dfs[i].columns), cp.array(short_dfs[i].values))\n",
    "            f.write(f\"Reduction to {feature_count} features.\\nBest error: {error_history[-1]}.\\nError history: {error_history}.\\n\\\n",
    "                    Best solution: {best_solution}.\\nSelected subset: {[short_dfs[i].columns[j] for j in np.where(best_solution == 1)[0]]}.\\n\\n\")\n",
    "            if error_history[-1] == 0:\n",
    "                break\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6013863,
     "sourceId": 9810302,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6246783,
     "sourceId": 10123240,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6250860,
     "sourceId": 10128920,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6251900,
     "sourceId": 10130311,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
