{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"../../dataset\"\n",
    "\n",
    "directory = os.fsencode(dataset_path)\n",
    "file_list = os.listdir(directory)\n",
    "dfs = []\n",
    "\n",
    "for file in file_list:\n",
    "    filename = os.fsdecode(file)\n",
    "    dfs.append(pd.read_csv(f\"{dataset_path}\\{filename}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1 nan values in df 13.\n",
      "We have 1 nan values in df 21.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dfs)):\n",
    "    nan_number = dfs[i].isna().sum().sum()\n",
    "    if nan_number > 0:\n",
    "        print(f\"We have {nan_number} nan values in df {i}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[13].dropna(inplace=True)\n",
    "dfs[21].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def df_transformation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.drop([\"Name\", \"LongName\", \"Parent\", \"Component\", \"Path\", \"Line\", \"Column\", \"EndLine\", \"EndColumn\", \"ID\"],\n",
    "            axis=1, inplace=True)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit_transform(df)\n",
    "    return df\n",
    "\n",
    "for df in dfs:\n",
    "    df = df_transformation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_population(num_food_sources, num_features, feature_count):\n",
    "    rng = np.random.default_rng()\n",
    "    population = np.zeros((num_food_sources, num_features), dtype=int)\n",
    "\n",
    "    for i in range(num_food_sources):\n",
    "        # randomly select feature_count indices to set to 1\n",
    "        active_indices = rng.choice(num_features, size=feature_count, replace=False)\n",
    "        population[i, active_indices] = 1\n",
    "\n",
    "    return population\n",
    "\n",
    "# used for generation of solutions during the mutation process\n",
    "def jaccard_dissimilarity(X1, X2):\n",
    "    intersection = np.sum(np.logical_and(X1, X2))\n",
    "    union = np.sum(np.logical_or(X1, X2))\n",
    "    return 1 - intersection / union if union > 0 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differential_mutation(food_sources, feature_count, phi, rng):\n",
    "    num_sources = len(food_sources)\n",
    "    mutant_sources = np.zeros_like(food_sources)\n",
    "    \n",
    "    for i in range(num_sources):\n",
    "        # choose 3 random neighbours\n",
    "        r1, r2, r3 =  rng.choice([j for j in range(num_sources) if j != i], size=3, replace=False)\n",
    "        Xr1, Xr2, Xr3 = food_sources[r1], food_sources[r2], food_sources[r3]\n",
    "\n",
    "        # calculate scaled jackard dissimilarity\n",
    "        dissimilarity_r2_r3 = jaccard_dissimilarity(Xr2, Xr3)\n",
    "        target_dissimilarity = phi * dissimilarity_r2_r3\n",
    "\n",
    "        # estimate similarity between new solution and r1\n",
    "        m1 = np.sum(Xr1)\n",
    "        m0 = len(Xr1) - m1\n",
    "\n",
    "        # estimate optimal ms\n",
    "        best_M11, best_M10, best_M01 = 0, 0, 0\n",
    "        min_difference = float('inf')\n",
    "\n",
    "        for M11 in range(m1 + 1):\n",
    "            for M10 in range(m0+1):\n",
    "                M01 = m1 - M11\n",
    "                denominator = M11 + M10 + M01\n",
    "                dissimilarity = 1 - (M11 / denominator if denominator != 0 else 1)\n",
    "                difference = abs(dissimilarity - target_dissimilarity)\n",
    "\n",
    "                if difference < min_difference:\n",
    "                    best_M11, best_M10, best_M01 = M11, M10, M01\n",
    "                    min_difference = difference\n",
    "\n",
    "        # compose le mutant\n",
    "        omega_i = np.zeros(len(food_sources[i]), dtype=int)\n",
    "\n",
    "        active_indices = np.where(Xr1 == 1)[0]\n",
    "        if len(active_indices) >= best_M11:\n",
    "            selected_indices = rng.choice(active_indices, size=best_M11, replace=False)\n",
    "            omega_i[selected_indices] = 1\n",
    "\n",
    "        inactive_indices = np.where(Xr1 == 0)[0]\n",
    "        if len(inactive_indices) >= best_M10:\n",
    "            selected_indices = rng.choice(inactive_indices, size=best_M10, replace=False)\n",
    "            omega_i[selected_indices] = 1\n",
    "\n",
    "        current_active_count = np.sum(omega_i)\n",
    "\n",
    "        if current_active_count < feature_count:\n",
    "            remaining_inactive_indices = np.where(omega_i == 0)[0]\n",
    "            additional_indices = rng.choice(remaining_inactive_indices, size=feature_count - current_active_count, replace=False)\n",
    "            omega_i[additional_indices] = 1\n",
    "\n",
    "        elif current_active_count > feature_count:\n",
    "            excess_active_indices = np.where(omega_i == 1)[0]\n",
    "            removal_indices = rng.choice(excess_active_indices, size=current_active_count - feature_count, replace=False)\n",
    "            omega_i[removal_indices] = 0\n",
    "\n",
    "        mutant_sources[i] = omega_i\n",
    "        # print(mutant_sources)\n",
    "\n",
    "    return mutant_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(parent, mutant, crossover_rate, rng, num_features):\n",
    "    offspring = np.where(rng.random(len(parent)) < crossover_rate, mutant, parent)\n",
    "    current_active_count = np.sum(offspring)\n",
    "    if current_active_count < num_features:\n",
    "        inactive_indices = np.where(offspring == 0)[0]\n",
    "        additional_indices = rng.choice(inactive_indices, size=num_features - current_active_count, replace=False)\n",
    "        offspring[additional_indices] = 1\n",
    "    elif current_active_count > num_features:\n",
    "        active_indices = np.where(offspring == 1)[0]\n",
    "        removal_indices = rng.choice(active_indices, size=current_active_count - num_features, replace=False)\n",
    "        offspring[removal_indices] = 0\n",
    "\n",
    "    # print(offspring)\n",
    "    return offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def optimized_sammon_error(high_distances, low_dim_data):\n",
    "    low_distances = np.linalg.norm(low_dim_data[:, None] - low_dim_data, axis=2)\n",
    "    high_dist_sum = np.sum(high_distances)\n",
    "    sammon_error_value = np.sum(((high_distances - low_distances) ** 2) / (high_distances + 1e-9)) / high_dist_sum\n",
    "    return sammon_error_value\n",
    "\n",
    "def reduce_features(data, binary_vector):\n",
    "    selected_features = data[:, binary_vector == 1]\n",
    "    return selected_features\n",
    "\n",
    "def mdisabc(num_food_sources, crossover_rate, phi, MAX_LIMIT, max_iterations, feature_count, num_features, dataset: pd.DataFrame):\n",
    "    rng = np.random.default_rng()\n",
    "    food_sources = initialize_population(num_food_sources, num_features, feature_count)\n",
    "    limits = np.zeros(num_food_sources)\n",
    "    best_solution = None\n",
    "    best_error = float('inf')\n",
    "    error_history = []\n",
    "    dataset_norms = np.linalg.norm(dataset[:, None] - dataset, axis=2)\n",
    "    low_dim_errors = np.zeros(num_food_sources)\n",
    "    sammon_errors = {}\n",
    "\n",
    "    for iteration in tqdm(range(max_iterations)):\n",
    "        mutants = differential_mutation(food_sources, feature_count, phi, rng)\n",
    "        for i in range(num_food_sources):\n",
    "            if tuple(food_sources[i]) not in sammon_errors:\n",
    "                subset_data = reduce_features(dataset, food_sources[i])\n",
    "                current_error = optimized_sammon_error(dataset_norms, subset_data)\n",
    "                sammon_errors[tuple(food_sources[i])] = current_error\n",
    "                # print(current_error)\n",
    "            else:\n",
    "                current_error = sammon_errors[tuple(food_sources[i])]\n",
    "\n",
    "            mutant = mutants[i]\n",
    "            candidate_solution = crossover(food_sources[i], mutant, crossover_rate, rng, feature_count)\n",
    "            \n",
    "            if tuple(candidate_solution) not in sammon_errors:\n",
    "                neighbor_subset_data = reduce_features(dataset, candidate_solution)\n",
    "                neighbor_error = optimized_sammon_error(dataset_norms, neighbor_subset_data)\n",
    "                sammon_errors[tuple(candidate_solution)] = neighbor_error\n",
    "            else:\n",
    "                neighbor_error = sammon_errors[tuple(candidate_solution)]\n",
    "\n",
    "            if neighbor_error < current_error:\n",
    "                food_sources[i] = candidate_solution\n",
    "                low_dim_errors[i] = neighbor_error\n",
    "                limits[i] = 0\n",
    "            else:\n",
    "                limits[i] += 1\n",
    "            # print(food_sources[i])\n",
    "\n",
    "            if current_error < best_error:\n",
    "                best_solution = food_sources[i]\n",
    "                best_error = current_error\n",
    "\n",
    "        for i in range(num_food_sources):\n",
    "            if limits[i] >= MAX_LIMIT:\n",
    "                pos_indices = rng.choice(num_features, size=feature_count, replace=False)\n",
    "                food_sources[i]=np.array([1 if i in pos_indices else 0 for i in range(num_features)])\n",
    "                limits[i] = 0\n",
    "\n",
    "        error_history.append(best_error)\n",
    "\n",
    "    return best_solution, error_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:20<00:00,  2.40it/s]\n",
      "100%|██████████| 50/50 [00:59<00:00,  1.19s/it]\n",
      "100%|██████████| 50/50 [01:54<00:00,  2.29s/it]\n",
      "100%|██████████| 50/50 [02:36<00:00,  3.13s/it]\n",
      "100%|██████████| 50/50 [03:28<00:00,  4.16s/it]\n",
      "100%|██████████| 50/50 [04:09<00:00,  4.98s/it]\n",
      "100%|██████████| 50/50 [04:40<00:00,  5.61s/it]\n",
      "100%|██████████| 50/50 [05:20<00:00,  6.41s/it]\n",
      "100%|██████████| 50/50 [05:55<00:00,  7.10s/it]\n",
      "100%|██████████| 50/50 [09:12<00:00, 11.05s/it]\n",
      "100%|██████████| 50/50 [10:15<00:00, 12.31s/it]\n",
      "100%|██████████| 50/50 [11:20<00:00, 13.61s/it]\n",
      "100%|██████████| 50/50 [11:57<00:00, 14.36s/it]\n",
      "100%|██████████| 50/50 [12:24<00:00, 14.88s/it]\n",
      "100%|██████████| 50/50 [12:48<00:00, 15.37s/it]\n",
      "100%|██████████| 50/50 [13:20<00:00, 16.00s/it]\n",
      "100%|██████████| 50/50 [13:33<00:00, 16.28s/it]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.70it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.54it/s]\n",
      "100%|██████████| 50/50 [00:10<00:00,  4.73it/s]\n",
      "100%|██████████| 50/50 [00:16<00:00,  3.11it/s]\n",
      "100%|██████████| 50/50 [00:22<00:00,  2.24it/s]\n",
      "100%|██████████| 50/50 [00:25<00:00,  1.93it/s]\n",
      "100%|██████████| 50/50 [00:29<00:00,  1.71it/s]\n",
      "100%|██████████| 50/50 [00:32<00:00,  1.55it/s]\n",
      "100%|██████████| 50/50 [00:36<00:00,  1.38it/s]\n",
      "100%|██████████| 50/50 [00:40<00:00,  1.25it/s]\n",
      "100%|██████████| 50/50 [00:42<00:00,  1.17it/s]\n",
      "100%|██████████| 50/50 [00:46<00:00,  1.09it/s]\n",
      "100%|██████████| 50/50 [00:49<00:00,  1.01it/s]\n",
      "100%|██████████| 50/50 [00:52<00:00,  1.05s/it]\n",
      "100%|██████████| 50/50 [00:54<00:00,  1.10s/it]\n",
      "100%|██████████| 50/50 [04:31<00:00,  5.43s/it]\n",
      "100%|██████████| 50/50 [13:33<00:00, 16.26s/it]\n",
      "100%|██████████| 50/50 [24:16<00:00, 29.13s/it] \n",
      "100%|██████████| 50/50 [33:11<00:00, 39.83s/it]  \n",
      "100%|██████████| 50/50 [43:42<00:00, 52.45s/it]  \n",
      "100%|██████████| 50/50 [52:31<00:00, 63.03s/it]  \n",
      "100%|██████████| 50/50 [1:02:15<00:00, 74.71s/it]\n",
      "100%|██████████| 50/50 [1:07:51<00:00, 81.44s/it] \n",
      "100%|██████████| 50/50 [48:51<00:00, 58.64s/it]  \n",
      "100%|██████████| 50/50 [50:55<00:00, 61.10s/it]  \n",
      "100%|██████████| 50/50 [1:00:05<00:00, 72.12s/it]\n",
      "100%|██████████| 50/50 [1:41:26<00:00, 121.74s/it]\n",
      "100%|██████████| 50/50 [1:15:28<00:00, 90.58s/it]\n",
      "100%|██████████| 50/50 [1:14:03<00:00, 88.87s/it]\n",
      " 18%|█▊        | 9/50 [15:39<1:05:09, 95.35s/it] "
     ]
    }
   ],
   "source": [
    "with open(\"outputs/log/abc.log\", \"a\", buffering=1) as f:\n",
    "    for i in range(11, len(dfs)):\n",
    "        f.write(f\"Working on {file_list[i]}...\\n\\n\")\n",
    "        for feature_count in range(2, len(dfs[i].columns)-1):\n",
    "            best_solution, error_history = mdisabc(30, 0.25, 0.9, 50, 50, feature_count, \n",
    "                                                   len(dfs[i].columns), dfs[i].values)\n",
    "            f.write(f\"Reduction to {feature_count} features.\\nBest error: {error_history[-1]}.\\nError history: {error_history}.\\n\\\n",
    "                    Best solution: {best_solution}.\\nSelected subset: {[dfs[i].columns[j] for j in np.where(best_solution == 1)[0]]}.\\n\\n\")\n",
    "            if error_history[-1] == 0:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:34:32<00:00, 113.46s/it]\n",
      "100%|██████████| 50/50 [1:14:41<00:00, 89.63s/it] \n",
      "100%|██████████| 50/50 [2:09:07<00:00, 154.95s/it]  \n",
      "100%|██████████| 50/50 [1:50:56<00:00, 133.13s/it]\n",
      "100%|██████████| 50/50 [1:41:35<00:00, 121.90s/it]  \n",
      "100%|██████████| 50/50 [1:39:33<00:00, 119.47s/it]\n",
      "100%|██████████| 50/50 [1:29:23<00:00, 107.28s/it]\n",
      "100%|██████████| 50/50 [1:42:55<00:00, 123.52s/it]\n",
      "100%|██████████| 50/50 [1:14:49<00:00, 89.78s/it]\n"
     ]
    }
   ],
   "source": [
    "with open(\"outputs/log/abc.log\", \"a\", buffering=1) as f:\n",
    "    for feature_count in range(10, len(dfs[10].columns)-1):\n",
    "        best_solution, error_history = mdisabc(30, 0.25, 0.9, 50, 50, feature_count, \n",
    "                                                len(dfs[10].columns), dfs[10].values)\n",
    "        f.write(f\"Reduction to {feature_count} features.\\nBest error: {error_history[-1]}.\\nError history: {error_history}.\\n\\\n",
    "                Best solution: {best_solution}.\\nSelected subset: {[dfs[10].columns[j] for j in np.where(best_solution == 1)[0]]}.\\n\\n\")\n",
    "        if error_history[-1] == 0:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
