{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../dataset/old_complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 100 repos in total\n"
     ]
    }
   ],
   "source": [
    "def check_dataset(dataset_folder_path):\n",
    "    \"\"\"\n",
    "    This function checks the dataset folder and prints the total number of files within it.\n",
    "\n",
    "    Args:\n",
    "    - dataset_folder_path (str): The path to the dataset folder containing multiple metric files from different repositories.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    folders_in_dataset = set(os.listdir(dataset_folder_path))\n",
    "\n",
    "    # info print\n",
    "    print(f\"Dataset has {len(folders_in_dataset)} repos in total\")\n",
    "    return None\n",
    "\n",
    "check_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"..\\..\\dataset\\new\\python-poetry_poetry_metrics.csv\")\n",
    "def df_transformation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms the given DataFrame by dropping unnecessary columns.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): Input DataFrame containing multiple columns.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Transformed DataFrame with specified columns removed.\n",
    "    \"\"\"\n",
    "    df.drop([\"Unnamed: 0\", \"Name\", \"LongName\", \"Parent\", \"Component\", \"Path\", \"Line\", \"Column\", \"EndLine\", \"EndColumn\", \"ID\"],\n",
    "            axis=1, inplace=True)\n",
    "    return df\n",
    "trans = df_transformation(df)\n",
    "columns_list = list(df.columns)\n",
    "\n",
    "# exclude the 0 variance columns that happen in all repos\n",
    "global_columns_with_variance = set()\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        df = pl.read_csv(file_path, schema_overrides={col: pl.Float32 for col in columns_list})\n",
    "        df = df.drop([\"\", \"ID\",\"Name\",\"LongName\",\"Parent\",\"Component\",\"Path\",\"Line\",\"Column\",\"EndLine\",\"EndColumn\"])\n",
    "        for col in columns_list:\n",
    "            if df[col].var() > 0:\n",
    "                global_columns_with_variance.add(col)\n",
    "METHOD_METRICS_LIST = list(global_columns_with_variance)\n",
    "zero_col_list = [metric for metric in columns_list if metric not in METHOD_METRICS_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_stats(folder_path):\n",
    "    \"\"\"\n",
    "    Gathers statistics on null and NaN values for specific metrics in CSV files within a folder structure.\n",
    "\n",
    "    Args:\n",
    "    - folder_path (str): Path to the root folder containing the datasets.\n",
    "\n",
    "    Returns:\n",
    "    - pl.DataFrame: A DataFrame containing the count of null and NaN values for each metric in the files processed,\n",
    "      along with the repository name.\n",
    "    \"\"\"\n",
    "    suffix = \"_metrics.csv\"\n",
    "    results = []\n",
    "    columns_list = METHOD_METRICS_LIST\n",
    "    num_columns = len(columns_list)\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(suffix):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pl.read_csv(file_path, columns=columns_list, schema_overrides=[pl.Float32]*num_columns)\n",
    "                if len(df) == 0:\n",
    "                    print(f\"Skipping empty {file}\")\n",
    "                    continue\n",
    "                repo_name = file.split('_metrics')[0] # only repo name\n",
    "                nulls_list = df.null_count().row(0)\n",
    "                try:\n",
    "                    nans_list = df.select(pl.all().is_nan().sum()).row(0)\n",
    "                except Exception as e:\n",
    "                    print(repo_name)\n",
    "                    print(df.schema)\n",
    "                    print(e)\n",
    "                results.append((repo_name, *nulls_list, *nans_list))\n",
    "\n",
    "    null_colnames = [x+\"_nulls\" for x in columns_list]\n",
    "    nan_colnames = [x+\"_NANs\" for x in columns_list]\n",
    "    df_schema = [\"repo_name\", *null_colnames, *nan_colnames]\n",
    "    res_df = pl.DataFrame(results, schema=df_schema, orient=\"row\")\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method metrics null analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_stats_methods = get_datasets_stats(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (100, 51)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>repo_name</th><th>HDIF_nulls</th><th>HVOL_nulls</th><th>Complexity Metric Rules_nulls</th><th>HTRP_nulls</th><th>CLOC_nulls</th><th>WarningInfo_nulls</th><th>LLOC_nulls</th><th>DLOC_nulls</th><th>Coupling Metric Rules_nulls</th><th>NOI_nulls</th><th>HNDB_nulls</th><th>TLOC_nulls</th><th>TCLOC_nulls</th><th>HEFF_nulls</th><th>NOS_nulls</th><th>Size Metric Rules_nulls</th><th>McCC_nulls</th><th>LOC_nulls</th><th>NUMPAR_nulls</th><th>TNOS_nulls</th><th>HPL_nulls</th><th>TLLOC_nulls</th><th>HPV_nulls</th><th>Anti Pattern_nulls</th><th>HCPL_nulls</th><th>HDIF_NANs</th><th>HVOL_NANs</th><th>Complexity Metric Rules_NANs</th><th>HTRP_NANs</th><th>CLOC_NANs</th><th>WarningInfo_NANs</th><th>LLOC_NANs</th><th>DLOC_NANs</th><th>Coupling Metric Rules_NANs</th><th>NOI_NANs</th><th>HNDB_NANs</th><th>TLOC_NANs</th><th>TCLOC_NANs</th><th>HEFF_NANs</th><th>NOS_NANs</th><th>Size Metric Rules_NANs</th><th>McCC_NANs</th><th>LOC_NANs</th><th>NUMPAR_NANs</th><th>TNOS_NANs</th><th>HPL_NANs</th><th>TLLOC_NANs</th><th>HPV_NANs</th><th>Anti Pattern_NANs</th><th>HCPL_NANs</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;aio-libs_aiohttp&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;aleju_imgaug&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;ansible_ansible&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;apache_tvm&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;AUTOMATIC1111_stable-diffusion…</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;unifyai_ivy&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;XX-net_XX-Net&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;yt-dlp_yt-dlp&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;ytdl-org_youtube-dl&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>&quot;zhayujie_chatgpt-on-wechat&quot;</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (100, 51)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬──────────┬───────────┬───────────┐\n",
       "│ repo_name ┆ HDIF_null ┆ HVOL_null ┆ Complexit ┆ … ┆ TLLOC_NAN ┆ HPV_NANs ┆ Anti Patt ┆ HCPL_NANs │\n",
       "│ ---       ┆ s         ┆ s         ┆ y Metric  ┆   ┆ s         ┆ ---      ┆ ern_NANs  ┆ ---       │\n",
       "│ str       ┆ ---       ┆ ---       ┆ Rules_nul ┆   ┆ ---       ┆ i64      ┆ ---       ┆ i64       │\n",
       "│           ┆ i64       ┆ i64       ┆ ls        ┆   ┆ i64       ┆          ┆ i64       ┆           │\n",
       "│           ┆           ┆           ┆ ---       ┆   ┆           ┆          ┆           ┆           │\n",
       "│           ┆           ┆           ┆ i64       ┆   ┆           ┆          ┆           ┆           │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪══════════╪═══════════╪═══════════╡\n",
       "│ aio-libs_ ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0        ┆ 0         ┆ 0         │\n",
       "│ aiohttp   ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
       "│ aleju_img ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0        ┆ 0         ┆ 0         │\n",
       "│ aug       ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
       "│ ansible_a ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0        ┆ 0         ┆ 0         │\n",
       "│ nsible    ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
       "│ apache_tv ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0        ┆ 0         ┆ 0         │\n",
       "│ m         ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
       "│ AUTOMATIC ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0        ┆ 0         ┆ 0         │\n",
       "│ 1111_stab ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
       "│ le-diffus ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
       "│ ion…      ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …        ┆ …         ┆ …         │\n",
       "│ unifyai_i ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0        ┆ 0         ┆ 0         │\n",
       "│ vy        ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
       "│ XX-net_XX ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0        ┆ 0         ┆ 0         │\n",
       "│ -Net      ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
       "│ yt-dlp_yt ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0        ┆ 0         ┆ 0         │\n",
       "│ -dlp      ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
       "│ ytdl-org_ ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0        ┆ 0         ┆ 0         │\n",
       "│ youtube-d ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
       "│ l         ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
       "│ zhayujie_ ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0        ┆ 0         ┆ 0         │\n",
       "│ chatgpt-o ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
       "│ n-wechat  ┆           ┆           ┆           ┆   ┆           ┆          ┆           ┆           │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴──────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_null_stats_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum over columns, exclude first reponame column, sum\n",
    "max_sum = df_null_stats_methods.select(\n",
    "    pl.all().exclude(\"repo_name\").sum()\n",
    ").row(0)[0]\n",
    "max_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_save_per_repo_correlations(folder_path):\n",
    "    \"\"\"\n",
    "    Calculates and saves correlation matrices for each repository's metrics data.\n",
    "\n",
    "    Args:\n",
    "    - folder_path (str): Path to the root folder containing metric files for repositories.\n",
    "\n",
    "    Returns:\n",
    "    - correlations (list): List of correlation matrices (as pandas DataFrames) for each repository.\n",
    "    - lengths (list): List of row counts (lengths) of the data for each repository processed.\n",
    "    \"\"\"\n",
    "    suffix = \"_metrics.csv\"\n",
    "    columns_list = [x for x in METHOD_METRICS_LIST]\n",
    "    correlations = []\n",
    "    lengths = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(suffix):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pl.read_csv(file_path, columns=columns_list, schema_overrides={col: pl.Float32 for col in columns_list})\n",
    "                df = df.with_columns(\n",
    "                    [pl.col(col).fill_nan(0).alias(col) for col in df.columns]\n",
    "                )\n",
    "                df = df.with_columns(\n",
    "                    [pl.col(col).fill_null(0).alias(col) for col in df.columns]\n",
    "                )\n",
    "                # df = df.select([col for col in df.columns if df[col].var() > 0])\n",
    "                # ensure no full zero columns remain\n",
    "                df = df.with_columns(\n",
    "                    [pl.col(col) + np.random.normal(0, 1e-6, len(df)) for col in df.columns]\n",
    "                )\n",
    "                if len(df) == 0:\n",
    "                    print(f\"Skipping empty {file}\")\n",
    "                    continue\n",
    "                if len(df) == 1:\n",
    "                    print(f\"Skipping oneline {file}\")\n",
    "                    continue\n",
    "                repo_name = file.split('_metrics')[0] # only repo name\n",
    "                try:\n",
    "                    df_corr = df.corr().to_pandas() #.fill_nan(0).cast(pl.Float32) # XXX\n",
    "                    os.makedirs(\"outputs/file_correlations\", exist_ok=True)\n",
    "                    filepath = f\"outputs/file_correlations/{file}_correlations.csv\"\n",
    "                    df_corr.to_csv(filepath)\n",
    "                    correlations.append(df_corr)\n",
    "                    lengths.append(df.shape[0])\n",
    "                except Exception as e:\n",
    "                    print(repo_name)\n",
    "                    print(df_corr)\n",
    "                    # print(df.schema)\n",
    "                    print(e)\n",
    "\n",
    "    return correlations, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_df_list, lengths_list = calculate_and_save_per_repo_correlations(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = correlations_df_list[0].columns\n",
    "\n",
    "for df in correlations_df_list:\n",
    "    for col in df.columns:\n",
    "        if col not in cols:\n",
    "            print(col)\n",
    "            print(df.head(5))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLOC</th>\n",
       "      <th>DLOC</th>\n",
       "      <th>LLOC</th>\n",
       "      <th>LOC</th>\n",
       "      <th>McCC</th>\n",
       "      <th>NOI</th>\n",
       "      <th>NOS</th>\n",
       "      <th>NUMPAR</th>\n",
       "      <th>TCLOC</th>\n",
       "      <th>TLLOC</th>\n",
       "      <th>...</th>\n",
       "      <th>Coupling Metric Rules</th>\n",
       "      <th>Size Metric Rules</th>\n",
       "      <th>HCPL</th>\n",
       "      <th>HDIF</th>\n",
       "      <th>HEFF</th>\n",
       "      <th>HNDB</th>\n",
       "      <th>HPL</th>\n",
       "      <th>HPV</th>\n",
       "      <th>HTRP</th>\n",
       "      <th>HVOL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.771421</td>\n",
       "      <td>0.217951</td>\n",
       "      <td>0.451634</td>\n",
       "      <td>0.327449</td>\n",
       "      <td>0.129194</td>\n",
       "      <td>0.335043</td>\n",
       "      <td>0.201601</td>\n",
       "      <td>0.999365</td>\n",
       "      <td>0.218133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011119</td>\n",
       "      <td>0.097587</td>\n",
       "      <td>0.173317</td>\n",
       "      <td>0.232343</td>\n",
       "      <td>0.141760</td>\n",
       "      <td>0.153349</td>\n",
       "      <td>0.172176</td>\n",
       "      <td>0.198299</td>\n",
       "      <td>0.141760</td>\n",
       "      <td>0.153350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.771421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079896</td>\n",
       "      <td>0.272275</td>\n",
       "      <td>0.228429</td>\n",
       "      <td>0.060025</td>\n",
       "      <td>0.146748</td>\n",
       "      <td>0.203328</td>\n",
       "      <td>0.770200</td>\n",
       "      <td>0.080467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018055</td>\n",
       "      <td>0.114961</td>\n",
       "      <td>0.075147</td>\n",
       "      <td>0.114098</td>\n",
       "      <td>0.059984</td>\n",
       "      <td>0.069279</td>\n",
       "      <td>0.078494</td>\n",
       "      <td>0.089293</td>\n",
       "      <td>0.059984</td>\n",
       "      <td>0.069279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.217951</td>\n",
       "      <td>0.079896</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961856</td>\n",
       "      <td>0.394928</td>\n",
       "      <td>0.220735</td>\n",
       "      <td>0.770537</td>\n",
       "      <td>0.032588</td>\n",
       "      <td>0.217960</td>\n",
       "      <td>0.999147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009031</td>\n",
       "      <td>0.341225</td>\n",
       "      <td>0.398641</td>\n",
       "      <td>0.282946</td>\n",
       "      <td>0.365925</td>\n",
       "      <td>0.425500</td>\n",
       "      <td>0.413329</td>\n",
       "      <td>0.370809</td>\n",
       "      <td>0.365925</td>\n",
       "      <td>0.425498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.451634</td>\n",
       "      <td>0.272275</td>\n",
       "      <td>0.961856</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.457715</td>\n",
       "      <td>0.236566</td>\n",
       "      <td>0.810008</td>\n",
       "      <td>0.087061</td>\n",
       "      <td>0.451940</td>\n",
       "      <td>0.961582</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006740</td>\n",
       "      <td>0.344648</td>\n",
       "      <td>0.414771</td>\n",
       "      <td>0.316283</td>\n",
       "      <td>0.387435</td>\n",
       "      <td>0.438750</td>\n",
       "      <td>0.426459</td>\n",
       "      <td>0.389311</td>\n",
       "      <td>0.387435</td>\n",
       "      <td>0.438749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.327449</td>\n",
       "      <td>0.228429</td>\n",
       "      <td>0.394928</td>\n",
       "      <td>0.457715</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.210620</td>\n",
       "      <td>0.574123</td>\n",
       "      <td>0.332042</td>\n",
       "      <td>0.326557</td>\n",
       "      <td>0.395914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021704</td>\n",
       "      <td>0.333632</td>\n",
       "      <td>0.596449</td>\n",
       "      <td>0.554672</td>\n",
       "      <td>0.682334</td>\n",
       "      <td>0.637604</td>\n",
       "      <td>0.594694</td>\n",
       "      <td>0.554893</td>\n",
       "      <td>0.682334</td>\n",
       "      <td>0.637607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CLOC      DLOC      LLOC       LOC      McCC       NOI       NOS  \\\n",
       "0  1.000000  0.771421  0.217951  0.451634  0.327449  0.129194  0.335043   \n",
       "1  0.771421  1.000000  0.079896  0.272275  0.228429  0.060025  0.146748   \n",
       "2  0.217951  0.079896  1.000000  0.961856  0.394928  0.220735  0.770537   \n",
       "3  0.451634  0.272275  0.961856  1.000000  0.457715  0.236566  0.810008   \n",
       "4  0.327449  0.228429  0.394928  0.457715  1.000000  0.210620  0.574123   \n",
       "\n",
       "     NUMPAR     TCLOC     TLLOC  ...  Coupling Metric Rules  \\\n",
       "0  0.201601  0.999365  0.218133  ...               0.011119   \n",
       "1  0.203328  0.770200  0.080467  ...               0.018055   \n",
       "2  0.032588  0.217960  0.999147  ...              -0.009031   \n",
       "3  0.087061  0.451940  0.961582  ...              -0.006740   \n",
       "4  0.332042  0.326557  0.395914  ...              -0.021704   \n",
       "\n",
       "   Size Metric Rules      HCPL      HDIF      HEFF      HNDB       HPL  \\\n",
       "0           0.097587  0.173317  0.232343  0.141760  0.153349  0.172176   \n",
       "1           0.114961  0.075147  0.114098  0.059984  0.069279  0.078494   \n",
       "2           0.341225  0.398641  0.282946  0.365925  0.425500  0.413329   \n",
       "3           0.344648  0.414771  0.316283  0.387435  0.438750  0.426459   \n",
       "4           0.333632  0.596449  0.554672  0.682334  0.637604  0.594694   \n",
       "\n",
       "        HPV      HTRP      HVOL  \n",
       "0  0.198299  0.141760  0.153350  \n",
       "1  0.089293  0.059984  0.069279  \n",
       "2  0.370809  0.365925  0.425498  \n",
       "3  0.389311  0.387435  0.438749  \n",
       "4  0.554893  0.682334  0.637607  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_df_list[5].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CLOC', 'DLOC', 'LLOC', 'LOC', 'McCC', 'NOI', 'NOS', 'NUMPAR', 'TCLOC',\n",
       "       'TLLOC', 'TLOC', 'TNOS', 'WarningInfo', 'Anti Pattern',\n",
       "       'Complexity Metric Rules', 'Coupling Metric Rules', 'Size Metric Rules',\n",
       "       'HCPL', 'HDIF', 'HEFF', 'HNDB', 'HPL', 'HPV', 'HTRP', 'HVOL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_df_list[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1573"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_correlations(folder_path):\n",
    "    \"\"\"\n",
    "    Combines datasets from multiple repositories into a single DataFrame for correlation analysis.\n",
    "\n",
    "    Args:\n",
    "    - folder_path (str): Path to the root folder containing metrics CSV files for repositories.\n",
    "\n",
    "    Returns:\n",
    "    - pl.DataFrame: A concatenated DataFrame containing data from all repositories.\n",
    "    \"\"\"\n",
    "    suffix = \"_metrics.csv\"\n",
    "    columns_list = [x for x in METHOD_METRICS_LIST]\n",
    "    df_list = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(suffix):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pl.read_csv(file_path, columns=columns_list, schema_overrides={col: pl.Float32 for col in columns_list})\n",
    "                if len(df) == 0:\n",
    "                    print(f\"Skipping empty {file}\")\n",
    "                    continue\n",
    "                if len(df) == 1:\n",
    "                    print(f\"Skipping oneline {file}\")\n",
    "                    continue\n",
    "                df_list.append(df)\n",
    "\n",
    "    df_all = pl.concat(df_list)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (407_558, 25)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>CLOC</th><th>DLOC</th><th>LLOC</th><th>LOC</th><th>McCC</th><th>NOI</th><th>NOS</th><th>NUMPAR</th><th>TCLOC</th><th>TLLOC</th><th>TLOC</th><th>TNOS</th><th>WarningInfo</th><th>Anti Pattern</th><th>Complexity Metric Rules</th><th>Coupling Metric Rules</th><th>Size Metric Rules</th><th>HCPL</th><th>HDIF</th><th>HEFF</th><th>HNDB</th><th>HPL</th><th>HPV</th><th>HTRP</th><th>HVOL</th></tr><tr><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0.0</td><td>0.0</td><td>2.0</td><td>2.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>2.0</td><td>2.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>6.0</td><td>6.0</td><td>1.0</td><td>7.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>2.0</td><td>6.0</td><td>1.0</td><td>7.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>0.0</td><td>0.0</td><td>3.0</td><td>3.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>3.0</td><td>3.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>1.0</td><td>1.0</td><td>2.0</td><td>3.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>2.0</td><td>3.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>1.0</td><td>1.0</td><td>2.0</td><td>3.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>2.0</td><td>1.0</td><td>2.0</td><td>3.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0.0</td><td>0.0</td><td>12.0</td><td>12.0</td><td>2.0</td><td>0.0</td><td>9.0</td><td>2.0</td><td>0.0</td><td>12.0</td><td>12.0</td><td>9.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>4.0</td><td>0.0</td><td>10.0</td><td>14.0</td><td>4.0</td><td>0.0</td><td>8.0</td><td>1.0</td><td>4.0</td><td>10.0</td><td>14.0</td><td>8.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>10.0</td><td>1.0</td><td>15.509775</td><td>0.00517</td><td>6.0</td><td>6.0</td><td>0.861654</td><td>15.509775</td></tr><tr><td>8.0</td><td>0.0</td><td>17.0</td><td>30.0</td><td>5.0</td><td>0.0</td><td>13.0</td><td>2.0</td><td>8.0</td><td>17.0</td><td>30.0</td><td>13.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>81.096512</td><td>2.647059</td><td>318.718201</td><td>0.040135</td><td>27.0</td><td>22.0</td><td>17.706566</td><td>120.404655</td></tr><tr><td>3.0</td><td>3.0</td><td>2.0</td><td>5.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>2.0</td><td>3.0</td><td>2.0</td><td>5.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>3.0</td><td>3.0</td><td>2.0</td><td>5.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>2.0</td><td>3.0</td><td>2.0</td><td>5.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (407_558, 25)\n",
       "┌──────┬──────┬──────┬──────┬───┬──────┬──────┬───────────┬────────────┐\n",
       "│ CLOC ┆ DLOC ┆ LLOC ┆ LOC  ┆ … ┆ HPL  ┆ HPV  ┆ HTRP      ┆ HVOL       │\n",
       "│ ---  ┆ ---  ┆ ---  ┆ ---  ┆   ┆ ---  ┆ ---  ┆ ---       ┆ ---        │\n",
       "│ f32  ┆ f32  ┆ f32  ┆ f32  ┆   ┆ f32  ┆ f32  ┆ f32       ┆ f32        │\n",
       "╞══════╪══════╪══════╪══════╪═══╪══════╪══════╪═══════════╪════════════╡\n",
       "│ 0.0  ┆ 0.0  ┆ 2.0  ┆ 2.0  ┆ … ┆ 0.0  ┆ 0.0  ┆ 0.0       ┆ 0.0        │\n",
       "│ 6.0  ┆ 6.0  ┆ 1.0  ┆ 7.0  ┆ … ┆ 0.0  ┆ 0.0  ┆ 0.0       ┆ 0.0        │\n",
       "│ 0.0  ┆ 0.0  ┆ 3.0  ┆ 3.0  ┆ … ┆ 0.0  ┆ 0.0  ┆ 0.0       ┆ 0.0        │\n",
       "│ 1.0  ┆ 1.0  ┆ 2.0  ┆ 3.0  ┆ … ┆ 0.0  ┆ 0.0  ┆ 0.0       ┆ 0.0        │\n",
       "│ 1.0  ┆ 1.0  ┆ 2.0  ┆ 3.0  ┆ … ┆ 0.0  ┆ 0.0  ┆ 0.0       ┆ 0.0        │\n",
       "│ …    ┆ …    ┆ …    ┆ …    ┆ … ┆ …    ┆ …    ┆ …         ┆ …          │\n",
       "│ 0.0  ┆ 0.0  ┆ 12.0 ┆ 12.0 ┆ … ┆ 0.0  ┆ 0.0  ┆ 0.0       ┆ 0.0        │\n",
       "│ 4.0  ┆ 0.0  ┆ 10.0 ┆ 14.0 ┆ … ┆ 6.0  ┆ 6.0  ┆ 0.861654  ┆ 15.509775  │\n",
       "│ 8.0  ┆ 0.0  ┆ 17.0 ┆ 30.0 ┆ … ┆ 27.0 ┆ 22.0 ┆ 17.706566 ┆ 120.404655 │\n",
       "│ 3.0  ┆ 3.0  ┆ 2.0  ┆ 5.0  ┆ … ┆ 0.0  ┆ 0.0  ┆ 0.0       ┆ 0.0        │\n",
       "│ 3.0  ┆ 3.0  ┆ 2.0  ┆ 5.0  ┆ … ┆ 0.0  ┆ 0.0  ┆ 0.0       ┆ 0.0        │\n",
       "└──────┴──────┴──────┴──────┴───┴──────┴──────┴───────────┴────────────┘"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_dataset = dataset_correlations(dataset_path)\n",
    "df_all_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col for col in df_all_dataset.columns if df_all_dataset[col].n_unique() == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_dataset = df_all_dataset.select([col for col in df_all_dataset.columns if df_all_dataset[col].std() > 0])\n",
    "\n",
    "df_all_corr = df_all_dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully to outputs\\method-all-correlations.csv\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'outputs'\n",
    "file_path = os.path.join(output_dir, 'method-all-correlations.csv')\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "df_all_corr.write_csv(file_path)\n",
    "print(f\"File saved successfully to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (25, 25)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>CLOC</th><th>DLOC</th><th>LLOC</th><th>LOC</th><th>McCC</th><th>NOI</th><th>NOS</th><th>NUMPAR</th><th>TCLOC</th><th>TLLOC</th><th>TLOC</th><th>TNOS</th><th>WarningInfo</th><th>Anti Pattern</th><th>Complexity Metric Rules</th><th>Coupling Metric Rules</th><th>Size Metric Rules</th><th>HCPL</th><th>HDIF</th><th>HEFF</th><th>HNDB</th><th>HPL</th><th>HPV</th><th>HTRP</th><th>HVOL</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.0</td><td>0.925949</td><td>0.358402</td><td>0.615358</td><td>0.347203</td><td>0.163798</td><td>0.342106</td><td>0.301332</td><td>0.99591</td><td>0.349854</td><td>0.599397</td><td>0.331333</td><td>0.415082</td><td>0.338745</td><td>0.262441</td><td>0.047716</td><td>0.417071</td><td>0.21945</td><td>0.262351</td><td>0.105025</td><td>0.175628</td><td>0.220285</td><td>0.258241</td><td>0.105025</td><td>0.175628</td></tr><tr><td>0.925949</td><td>1.0</td><td>0.186624</td><td>0.447534</td><td>0.202257</td><td>0.094544</td><td>0.151737</td><td>0.279526</td><td>0.919936</td><td>0.179368</td><td>0.432634</td><td>0.145315</td><td>0.288092</td><td>0.252641</td><td>0.154334</td><td>0.017789</td><td>0.30179</td><td>0.120978</td><td>0.153175</td><td>0.048763</td><td>0.091285</td><td>0.117372</td><td>0.143639</td><td>0.048763</td><td>0.091285</td></tr><tr><td>0.358402</td><td>0.186624</td><td>1.0</td><td>0.950789</td><td>0.642688</td><td>0.246924</td><td>0.796353</td><td>0.373064</td><td>0.361877</td><td>0.979198</td><td>0.933538</td><td>0.773239</td><td>0.695159</td><td>0.560807</td><td>0.46601</td><td>0.087687</td><td>0.683244</td><td>0.468528</td><td>0.454307</td><td>0.253668</td><td>0.402623</td><td>0.472174</td><td>0.517346</td><td>0.253668</td><td>0.402623</td></tr><tr><td>0.615358</td><td>0.447534</td><td>0.950789</td><td>1.0</td><td>0.666492</td><td>0.271703</td><td>0.802531</td><td>0.403813</td><td>0.61755</td><td>0.934246</td><td>0.983013</td><td>0.781773</td><td>0.724997</td><td>0.582549</td><td>0.487801</td><td>0.092378</td><td>0.710985</td><td>0.473515</td><td>0.480283</td><td>0.251204</td><td>0.401322</td><td>0.477357</td><td>0.530756</td><td>0.251204</td><td>0.401322</td></tr><tr><td>0.347203</td><td>0.202257</td><td>0.642688</td><td>0.666492</td><td>1.0</td><td>0.285188</td><td>0.791915</td><td>0.268293</td><td>0.349888</td><td>0.632744</td><td>0.656692</td><td>0.767182</td><td>0.647761</td><td>0.549994</td><td>0.706462</td><td>0.120567</td><td>0.545926</td><td>0.565703</td><td>0.553567</td><td>0.30192</td><td>0.469049</td><td>0.550744</td><td>0.630765</td><td>0.30192</td><td>0.469049</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0.175628</td><td>0.091285</td><td>0.402623</td><td>0.401322</td><td>0.469049</td><td>0.112393</td><td>0.495992</td><td>0.144394</td><td>0.183114</td><td>0.415081</td><td>0.413036</td><td>0.504884</td><td>0.367185</td><td>0.274031</td><td>0.323266</td><td>0.052207</td><td>0.334843</td><td>0.933597</td><td>0.502085</td><td>0.848191</td><td>1.0</td><td>0.957956</td><td>0.81282</td><td>0.848191</td><td>1.0</td></tr><tr><td>0.220285</td><td>0.117372</td><td>0.472174</td><td>0.477357</td><td>0.550744</td><td>0.145733</td><td>0.578498</td><td>0.191706</td><td>0.228645</td><td>0.485495</td><td>0.489556</td><td>0.587013</td><td>0.435635</td><td>0.351116</td><td>0.400179</td><td>0.058119</td><td>0.390332</td><td>0.946988</td><td>0.690935</td><td>0.724138</td><td>0.957956</td><td>1.0</td><td>0.918976</td><td>0.724138</td><td>0.957956</td></tr><tr><td>0.258241</td><td>0.143639</td><td>0.517346</td><td>0.530756</td><td>0.630765</td><td>0.192324</td><td>0.624189</td><td>0.237951</td><td>0.268217</td><td>0.535963</td><td>0.547391</td><td>0.638926</td><td>0.480882</td><td>0.414957</td><td>0.47495</td><td>0.068326</td><td>0.418724</td><td>0.930406</td><td>0.803393</td><td>0.497699</td><td>0.81282</td><td>0.918976</td><td>1.0</td><td>0.497699</td><td>0.81282</td></tr><tr><td>0.105025</td><td>0.048763</td><td>0.253668</td><td>0.251204</td><td>0.30192</td><td>0.057584</td><td>0.346151</td><td>0.066667</td><td>0.109534</td><td>0.260179</td><td>0.25753</td><td>0.348645</td><td>0.213188</td><td>0.135698</td><td>0.168477</td><td>0.03297</td><td>0.201634</td><td>0.657036</td><td>0.270064</td><td>1.0</td><td>0.848191</td><td>0.724138</td><td>0.497699</td><td>1.0</td><td>0.848191</td></tr><tr><td>0.175628</td><td>0.091285</td><td>0.402623</td><td>0.401322</td><td>0.469049</td><td>0.112393</td><td>0.495992</td><td>0.144394</td><td>0.183114</td><td>0.415081</td><td>0.413036</td><td>0.504884</td><td>0.367185</td><td>0.274031</td><td>0.323266</td><td>0.052207</td><td>0.334843</td><td>0.933597</td><td>0.502085</td><td>0.848191</td><td>1.0</td><td>0.957956</td><td>0.81282</td><td>0.848191</td><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (25, 25)\n",
       "┌──────────┬──────────┬──────────┬──────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
       "│ CLOC     ┆ DLOC     ┆ LLOC     ┆ LOC      ┆ … ┆ HPL      ┆ HPV      ┆ HTRP     ┆ HVOL     │\n",
       "│ ---      ┆ ---      ┆ ---      ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
       "│ f64      ┆ f64      ┆ f64      ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
       "╞══════════╪══════════╪══════════╪══════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
       "│ 1.0      ┆ 0.925949 ┆ 0.358402 ┆ 0.615358 ┆ … ┆ 0.220285 ┆ 0.258241 ┆ 0.105025 ┆ 0.175628 │\n",
       "│ 0.925949 ┆ 1.0      ┆ 0.186624 ┆ 0.447534 ┆ … ┆ 0.117372 ┆ 0.143639 ┆ 0.048763 ┆ 0.091285 │\n",
       "│ 0.358402 ┆ 0.186624 ┆ 1.0      ┆ 0.950789 ┆ … ┆ 0.472174 ┆ 0.517346 ┆ 0.253668 ┆ 0.402623 │\n",
       "│ 0.615358 ┆ 0.447534 ┆ 0.950789 ┆ 1.0      ┆ … ┆ 0.477357 ┆ 0.530756 ┆ 0.251204 ┆ 0.401322 │\n",
       "│ 0.347203 ┆ 0.202257 ┆ 0.642688 ┆ 0.666492 ┆ … ┆ 0.550744 ┆ 0.630765 ┆ 0.30192  ┆ 0.469049 │\n",
       "│ …        ┆ …        ┆ …        ┆ …        ┆ … ┆ …        ┆ …        ┆ …        ┆ …        │\n",
       "│ 0.175628 ┆ 0.091285 ┆ 0.402623 ┆ 0.401322 ┆ … ┆ 0.957956 ┆ 0.81282  ┆ 0.848191 ┆ 1.0      │\n",
       "│ 0.220285 ┆ 0.117372 ┆ 0.472174 ┆ 0.477357 ┆ … ┆ 1.0      ┆ 0.918976 ┆ 0.724138 ┆ 0.957956 │\n",
       "│ 0.258241 ┆ 0.143639 ┆ 0.517346 ┆ 0.530756 ┆ … ┆ 0.918976 ┆ 1.0      ┆ 0.497699 ┆ 0.81282  │\n",
       "│ 0.105025 ┆ 0.048763 ┆ 0.253668 ┆ 0.251204 ┆ … ┆ 0.724138 ┆ 0.497699 ┆ 1.0      ┆ 0.848191 │\n",
       "│ 0.175628 ┆ 0.091285 ┆ 0.402623 ┆ 0.401322 ┆ … ┆ 0.957956 ┆ 0.81282  ┆ 0.848191 ┆ 1.0      │\n",
       "└──────────┴──────────┴──────────┴──────────┴───┴──────────┴──────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (600, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>index</th><th>variable</th><th>value</th></tr><tr><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;DLOC&quot;</td><td>&quot;CLOC&quot;</td><td>0.925949</td></tr><tr><td>&quot;LLOC&quot;</td><td>&quot;CLOC&quot;</td><td>0.358402</td></tr><tr><td>&quot;LOC&quot;</td><td>&quot;CLOC&quot;</td><td>0.615358</td></tr><tr><td>&quot;McCC&quot;</td><td>&quot;CLOC&quot;</td><td>0.347203</td></tr><tr><td>&quot;NOI&quot;</td><td>&quot;CLOC&quot;</td><td>0.163798</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;HEFF&quot;</td><td>&quot;HVOL&quot;</td><td>0.848191</td></tr><tr><td>&quot;HNDB&quot;</td><td>&quot;HVOL&quot;</td><td>1.0</td></tr><tr><td>&quot;HPL&quot;</td><td>&quot;HVOL&quot;</td><td>0.957956</td></tr><tr><td>&quot;HPV&quot;</td><td>&quot;HVOL&quot;</td><td>0.81282</td></tr><tr><td>&quot;HTRP&quot;</td><td>&quot;HVOL&quot;</td><td>0.848191</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (600, 3)\n",
       "┌───────┬──────────┬──────────┐\n",
       "│ index ┆ variable ┆ value    │\n",
       "│ ---   ┆ ---      ┆ ---      │\n",
       "│ str   ┆ str      ┆ f64      │\n",
       "╞═══════╪══════════╪══════════╡\n",
       "│ DLOC  ┆ CLOC     ┆ 0.925949 │\n",
       "│ LLOC  ┆ CLOC     ┆ 0.358402 │\n",
       "│ LOC   ┆ CLOC     ┆ 0.615358 │\n",
       "│ McCC  ┆ CLOC     ┆ 0.347203 │\n",
       "│ NOI   ┆ CLOC     ┆ 0.163798 │\n",
       "│ …     ┆ …        ┆ …        │\n",
       "│ HEFF  ┆ HVOL     ┆ 0.848191 │\n",
       "│ HNDB  ┆ HVOL     ┆ 1.0      │\n",
       "│ HPL   ┆ HVOL     ┆ 0.957956 │\n",
       "│ HPV   ┆ HVOL     ┆ 0.81282  │\n",
       "│ HTRP  ┆ HVOL     ┆ 0.848191 │\n",
       "└───────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_rows = (\n",
    "    df_all_dataset.corr()\n",
    "    .with_columns(pl.Series(name=\"index\", values=df_all_dataset.columns))\n",
    "    .unpivot(index = \"index\")\n",
    "    .filter(pl.col('index') != pl.col('variable'))\n",
    ")\n",
    "correlations_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (47, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>index</th><th>variable</th><th>value</th></tr><tr><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;HVOL&quot;</td><td>&quot;HNDB&quot;</td><td>1.0</td></tr><tr><td>&quot;HTRP&quot;</td><td>&quot;HEFF&quot;</td><td>1.0</td></tr><tr><td>&quot;TCLOC&quot;</td><td>&quot;CLOC&quot;</td><td>0.99591</td></tr><tr><td>&quot;LOC&quot;</td><td>&quot;TLOC&quot;</td><td>0.983013</td></tr><tr><td>&quot;TLLOC&quot;</td><td>&quot;LLOC&quot;</td><td>0.979198</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;HPL&quot;</td><td>&quot;HEFF&quot;</td><td>0.724138</td></tr><tr><td>&quot;WarningInfo&quot;</td><td>&quot;TLLOC&quot;</td><td>0.717936</td></tr><tr><td>&quot;TLOC&quot;</td><td>&quot;Size&nbsp;Metric&nbsp;Rules&quot;</td><td>0.716519</td></tr><tr><td>&quot;Size&nbsp;Metric&nbsp;Rules&quot;</td><td>&quot;LOC&quot;</td><td>0.710985</td></tr><tr><td>&quot;Complexity&nbsp;Metric&nbsp;Rules&quot;</td><td>&quot;McCC&quot;</td><td>0.706462</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (47, 3)\n",
       "┌─────────────────────────┬───────────────────┬──────────┐\n",
       "│ index                   ┆ variable          ┆ value    │\n",
       "│ ---                     ┆ ---               ┆ ---      │\n",
       "│ str                     ┆ str               ┆ f64      │\n",
       "╞═════════════════════════╪═══════════════════╪══════════╡\n",
       "│ HVOL                    ┆ HNDB              ┆ 1.0      │\n",
       "│ HTRP                    ┆ HEFF              ┆ 1.0      │\n",
       "│ TCLOC                   ┆ CLOC              ┆ 0.99591  │\n",
       "│ LOC                     ┆ TLOC              ┆ 0.983013 │\n",
       "│ TLLOC                   ┆ LLOC              ┆ 0.979198 │\n",
       "│ …                       ┆ …                 ┆ …        │\n",
       "│ HPL                     ┆ HEFF              ┆ 0.724138 │\n",
       "│ WarningInfo             ┆ TLLOC             ┆ 0.717936 │\n",
       "│ TLOC                    ┆ Size Metric Rules ┆ 0.716519 │\n",
       "│ Size Metric Rules       ┆ LOC               ┆ 0.710985 │\n",
       "│ Complexity Metric Rules ┆ McCC              ┆ 0.706462 │\n",
       "└─────────────────────────┴───────────────────┴──────────┘"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_rows_important = correlations_rows.filter(pl.col('value').abs() > 0.7).sort(by='value', descending=True).gather_every(2)\n",
    "correlations_rows_important.write_csv('outputs/method-all-important.csv')\n",
    "correlations_rows_important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_to_z(r_colname) -> pl.Expr:\n",
    "    \"\"\"\n",
    "    Applies the Fisher transformation to a column of correlation coefficients in a Polars DataFrame.\n",
    "\n",
    "    The Fisher transformation stabilizes the variance of correlation coefficients by converting\n",
    "    them into z-scores.\n",
    "\n",
    "    Args:\n",
    "    - r_colname (str): The name of the column containing correlation coefficients (r values).\n",
    "\n",
    "    Returns:\n",
    "    - pl.Expr: A Polars expression that computes the Fisher-transformed z-scores.\n",
    "    \"\"\"\n",
    "    return (0.5 * pl.Expr.log( (1 + pl.col(r_colname)) / (1 - pl.col(r_colname))))\n",
    "\n",
    "\n",
    "def variance_z(n_colname) -> pl.Expr:\n",
    "    \"\"\"\n",
    "    Computes the variance of the Fisher-transformed z-scores for correlation coefficients.\n",
    "\n",
    "    Args:\n",
    "    - n_colname (str): The column name in a Polars DataFrame representing the sample size (n).\n",
    "\n",
    "    Returns:\n",
    "    - pl.Expr: A Polars expression that calculates the variance of z-scores.\n",
    "    \"\"\"\n",
    "    return 1/(pl.col(n_colname) - 3)\n",
    "\n",
    "def z_to_r(z):\n",
    "    \"\"\"\n",
    "    Converts a Fisher-transformed z-score back to the Pearson correlation coefficient (r).\n",
    "\n",
    "    Args:\n",
    "    - z (float or numpy array): The Fisher-transformed z-score(s).\n",
    "\n",
    "    Returns:\n",
    "    - float or numpy array: The corresponding Pearson correlation coefficient(s).\n",
    "    \"\"\"\n",
    "    return (np.exp(2*z)-1)/(np.exp(2*z)+1)\n",
    "\n",
    "\n",
    "def std_z(n_colname) -> pl.Expr:\n",
    "    \"\"\"\n",
    "    Computes the standard deviation of Fisher-transformed z-scores for correlation coefficients.\n",
    "\n",
    "    Args:\n",
    "    - n_colname (str): The column name in a Polars DataFrame representing the sample size (n).\n",
    "\n",
    "    Returns:\n",
    "    - pl.Expr: A Polars expression that calculates the standard deviation of z-scores.\n",
    "    \"\"\"\n",
    "    return pl.Expr.sqrt(variance_z(n_colname))\n",
    "\n",
    "\n",
    "def making_fixed_effect_df(df):\n",
    "    \"\"\"\n",
    "    Prepares a DataFrame with columns needed for fixed-effects meta-analysis calculations.\n",
    "\n",
    "    Args:\n",
    "    - df (Polars DataFrame): Input DataFrame containing columns such as 'correlation' and 'n' (sample size).\n",
    "\n",
    "    Returns:\n",
    "    - Polars DataFrame: A DataFrame augmented with columns used in fixed-effects meta-analysis calculations.\n",
    "    \"\"\"\n",
    "    # transform input DataFrame by adding effect size and variance\n",
    "    df_fixed = df.with_columns(\n",
    "        # compute the effect size (Y) as the Fisher-transformed z-score of the 'correlation' column\n",
    "        effect_size_Y=r_to_z('correlation'),\n",
    "        # compute the within-study variance (V) as 1 / (n - 3), using the sample size 'n'\n",
    "        variance_within_V=variance_z('n'),\n",
    "    ).with_columns(\n",
    "        # compute the raw weight (W), which is the inverse of the within-study variance\n",
    "        raw_weight_W=1/pl.col('variance_within_V')\n",
    "    ).with_columns(\n",
    "        # calculate weighted effect size (WY) = W * Y\n",
    "        WY=(pl.col('raw_weight_W') * pl.col('effect_size_Y')),\n",
    "        # # calculate weighted squared effect size (WY^2) = W * Y^2\n",
    "        WY_2=(pl.col('raw_weight_W') * (pl.col('effect_size_Y'))**2),\n",
    "        # calculate squared weight (W^2)\n",
    "        W_2=(pl.col('raw_weight_W')**2),\n",
    "    )\n",
    "    return df_fixed\n",
    "\n",
    "def fixed_effect(df, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Conducts a fixed-effect meta-analysis on a given Polars DataFrame of correlations.\n",
    "\n",
    "    Args:\n",
    "    - df (Polars DataFrame): The input DataFrame containing correlation data.\n",
    "    - alpha (float, optional): The significance level for confidence intervals and hypothesis testing (default is 0.05).\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (p-value, fixed-effect correlation, lower confidence bound, upper confidence bound, significance flag)\n",
    "    \"\"\"\n",
    "    # prepare the dataframe for meta-analysis calculations\n",
    "    fixed_effect_df = making_fixed_effect_df(df)\n",
    "\n",
    "    # calculate the fixed-effect meta-analytic z-score (fixed_effect_z)\n",
    "    # this is the weighted average of the Fisher-transformed effect sizes (Y)\n",
    "    # weighted sum of effect sizes (sum(WY)) divided by the sum of weights (sum(W))\n",
    "    fixed_effect_z = fixed_effect_df.select(pl.sum('WY')/pl.sum('raw_weight_W')).item()\n",
    "\n",
    "    # calculate the variance of the fixed-effect z-score (fixed_variance_z)\n",
    "    # Variance is the inverse of the total weight (sum(W)).\n",
    "    fixed_variance_z = fixed_effect_df.select(1/pl.sum('raw_weight_W')).item()\n",
    "\n",
    "    # compute the std of the fixed-effect z-score (fixed_std_z)\n",
    "    # std is the square root of the variance.\n",
    "    fixed_std_z = np.sqrt(fixed_variance_z)\n",
    "\n",
    "    # determine the z-value for the confidence interval\n",
    "    # std_interval corresponds to the critical z-value for a two-tailed test at the given alpha.\n",
    "    std_interval = scipy.stats.norm.isf(alpha/2)\n",
    "\n",
    "    # compute the confidence interval in z-space (fixed_int_z)\n",
    "    # use the standard normal critical value (std_interval) to compute the interval\n",
    "    fixed_int_z = (fixed_effect_z-std_interval*fixed_std_z, fixed_effect_z+std_interval*fixed_std_z)\n",
    "\n",
    "    # compute the test statistic z-value for hypothesis testing\n",
    "    # z_value is the meta-analytic z-score divided by its standard deviation.\n",
    "    z_value = fixed_effect_z/fixed_std_z\n",
    "\n",
    "    # compute the p-value\n",
    "    # the p-value is twice the area in the tail of the standard normal distribution beyond |z_value|.\n",
    "    p_value = 2*scipy.stats.norm.sf(z_value)\n",
    "\n",
    "    # convert the fixed-effect z-score back to the correlation scale (r)\n",
    "    # the fixed-effect correlation coefficient is computed using the z_to_r function.\n",
    "    fixed_effect_cor = z_to_r(fixed_effect_z)\n",
    "\n",
    "    # convert the confidence interval from z-space back to the correlation scale\n",
    "    # Transform the lower and upper bounds of the z-confidence interval back to r.\n",
    "    fixed_int_corr = (z_to_r(fixed_int_z[0]), z_to_r(fixed_int_z[1]))\n",
    "    return (p_value, fixed_effect_cor, fixed_int_corr[0], fixed_int_corr[1], p_value <= alpha)\n",
    "\n",
    "def making_random_effects_df(df):\n",
    "    \"\"\"\n",
    "    Prepares a DataFrame for random-effects meta-analysis by incorporating between-study variance.\n",
    "\n",
    "    Args:\n",
    "    - df (Polars DataFrame): Input DataFrame containing correlation data and sample sizes.\n",
    "\n",
    "    Returns:\n",
    "    - Polars DataFrame: A DataFrame augmented with columns required for random-effects meta-analysis.\n",
    "    \"\"\"\n",
    "    # compute fixed-effect meta-analysis components\n",
    "    # use the fixed-effect meta-analysis preparation function to calculate:\n",
    "    # - Effect sizes (Y) and variances (V).\n",
    "    # - Weighted sums and other fixed-effect calculations\n",
    "    fixed_effect_df = making_fixed_effect_df(df)\n",
    "\n",
    "    # calculate the Q statistic\n",
    "    # Q statistic measures heterogeneity in effect sizes and is given by:\n",
    "    # Q = sum(WY^2) - (sum(WY)^2 / sum(W))\n",
    "    Q = fixed_effect_df.select(pl.sum('WY_2') - pl.sum('WY')**2 / pl.sum('raw_weight_W')).item()\n",
    "\n",
    "    # calculate the C constant\n",
    "    # C is a weighting term used in the random-effects model, calculated as:\n",
    "    # C = sum(W) - (sum(W^2) / sum(W))\n",
    "    C = fixed_effect_df.select(pl.sum('raw_weight_W') - pl.sum('W_2')/pl.sum('raw_weight_W')).item()\n",
    "\n",
    "    # compute between-study variance (T^2)\n",
    "    # T^2 is the between-study variance, also called tau-squared, and is given by:\n",
    "    # T^2 = (Q - (k - 1)) / C\n",
    "    # where k is the number of studies.\n",
    "    T_2 = (Q - (len(fixed_effect_df)-1))/C\n",
    "\n",
    "    fixed_effect_df = (\n",
    "        # update the variance (V_star) to account for between-study variance\n",
    "        # in random-effects meta-analysis, the total variance (V_star) includes both within-study\n",
    "        # variance (V) and between-study variance (T^2):\n",
    "        # V_star = V + T^2\n",
    "        fixed_effect_df.with_columns(V_star=pl.col('variance_within_V') + T_2)\n",
    "        # calculate updated weights (W_star)\n",
    "        # the new weights (W_star) are the inverse of the updated variances (V_star):\n",
    "        # W_star = 1 / V_star\n",
    "        .with_columns(W_star=1/pl.col('V_star'))\n",
    "        # compute the weighted effect sizes (W_star_Y)\n",
    "        # weighted effect sizes are updated using the new weights:\n",
    "        # W_star_Y = W_star * Y\n",
    "        .with_columns(W_star_Y=pl.col('W_star')*pl.col('effect_size_Y'))\n",
    "    )\n",
    "    return fixed_effect_df\n",
    "\n",
    "def random_effects(df, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Conducts random-effects meta-analysis on a given Polars DataFrame of correlations.\n",
    "\n",
    "    Args:\n",
    "    - df (Polars DataFrame): Input DataFrame containing correlation data and sample sizes.\n",
    "    - alpha (float, optional): The significance level for confidence intervals and hypothesis testing (default is 0.05).\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (p-value, pooled effect size, lower CI, upper CI, significance flag)\n",
    "    \"\"\"\n",
    "    # Prepare the DataFrame for random-effects meta-analysis\n",
    "    # the function 'making_random_effects_df' computes key components, including:\n",
    "    # - Between-study variance (T^2).\n",
    "    # - Updated variances (V*), weights (W*), and weighted effect sizes (W*Y).\n",
    "    random_effects_df = making_random_effects_df(df)\n",
    "\n",
    "    # calculate the pooled effect size in z-space\n",
    "    # the random-effects pooled effect size (random_effect_z) is the weighted average\n",
    "    # of Fisher-transformed z-scores using updated weights (W*):\n",
    "    # random_effect_z = sum(W*Y) / sum(W*)\n",
    "    random_effect_z = random_effects_df.select(pl.sum('W_star_Y')/pl.sum('W_star')).item()\n",
    "\n",
    "    # calculate the variance of the pooled effect size\n",
    "    # variance of the random-effects pooled effect size is the inverse of the sum of weights (W*):\n",
    "    # random_variance_z = 1 / sum(W*)\n",
    "    random_variance_z = random_effects_df.select(1/pl.sum('W_star')).item()\n",
    "\n",
    "    # compute the standard deviation\n",
    "    # standard deviation is the square root of the variance\n",
    "    random_std_z = np.sqrt(random_variance_z)\n",
    "\n",
    "    # determine the confidence interval in z-space\n",
    "    # calculate the critical z-value for the specified alpha level (two-tailed test)\n",
    "    # std_interval corresponds to the critical value of the standard normal distribution\n",
    "    std_interval = scipy.stats.norm.isf(alpha/2)\n",
    "\n",
    "    # compute the confidence interval for the pooled effect size in z-space:\n",
    "    # [random_effect_z - z_critical * std_dev, random_effect_z + z_critical * std_dev]\n",
    "    random_int_z = (random_effect_z-std_interval*random_std_z, random_effect_z+std_interval*random_std_z)\n",
    "\n",
    "    # perform the hypothesis testing\n",
    "    # compute the test statistic (z_value) for the pooled effect size:\n",
    "    # z_value = random_effect_z / std_dev\n",
    "    z_value = random_effect_z/random_std_z\n",
    "\n",
    "    # calculate the two-tailed p-value:\n",
    "    # p_value = 2 * P(Z > |z_value|)\n",
    "    p_value = 2*scipy.stats.norm.sf(z_value)\n",
    "\n",
    "    # convert pooled effect size back to correlation scale\n",
    "    # the pooled effect size is computed in z-space; transform it back to the correlation scale:\n",
    "    # random_effect_cor = z_to_r(random_effect_z).\n",
    "    random_effect_cor = z_to_r(random_effect_z)\n",
    "\n",
    "    # transform the confidence interval back to the correlation scale:\n",
    "    # random_int_corr = [z_to_r(lower_bound), z_to_r(upper_bound)].\n",
    "    random_int_corr = (z_to_r(random_int_z[0]), z_to_r(random_int_z[1]))\n",
    "    return (p_value, random_effect_cor, random_int_corr[0], random_int_corr[1], p_value <= alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Q_and_test(df, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculates the Q-statistic, performs a significance test for heterogeneity,\n",
    "    and returns whether the test was significant.\n",
    "\n",
    "    Args:\n",
    "    - df (Polars DataFrame): Input DataFrame containing correlation data and sample sizes.\n",
    "    - alpha (float, optional): Significance level for the Q-test (default is 0.05).\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (Q-statistic, p-value for Q, significance flag)\n",
    "    \"\"\"\n",
    "    # compute fixed-effect meta-analysis components\n",
    "    fixed_effect_df = making_fixed_effect_df(df)\n",
    "\n",
    "    # calculate the Q-statistic\n",
    "    Q = fixed_effect_df.select(pl.sum('WY_2') - pl.sum('WY') ** 2 / pl.sum('raw_weight_W')).item()\n",
    "\n",
    "    # degrees of freedom for Q test (k - 1)\n",
    "    k = len(fixed_effect_df)  # number of studies\n",
    "    df_Q = k - 1\n",
    "\n",
    "    # calculate the p-value for the Q-statistic\n",
    "    p_value_Q = 1 - scipy.stats.chi2.cdf(Q, df_Q)\n",
    "\n",
    "    # determine significance of the test\n",
    "    is_significant = p_value_Q <= alpha\n",
    "\n",
    "    # return the Q-statistic, p-value, and significance flag\n",
    "    return Q, p_value_Q, is_significant\n",
    "\n",
    "\n",
    "def count_significant_Qs(correlations_list, lengths_list, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Counts the number of significant and non-significant Q-statistics\n",
    "    for correlations between pairs of metrics in a list of correlation matrices.\n",
    "\n",
    "    Args:\n",
    "    - correlations_list (list of Polars DataFrame): List of correlation matrices.\n",
    "    - lengths_list (list of int): List of sample sizes corresponding to each correlation matrix.\n",
    "    - alpha (float, optional): Significance level for Q-test (default is 0.05).\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with counts of significant and non-significant Q-statistics.\n",
    "    \"\"\"\n",
    "    columns_names = correlations_list[0].columns  # column names of the correlation matrices\n",
    "    significant_count = 0  # count of significant Q-tests\n",
    "    not_significant_count = 0  # count of non-significant Q-tests\n",
    "\n",
    "    # iterate over each unique pair of metrics (columns)\n",
    "    for i in trange(len(columns_names)):\n",
    "        for j in range(i + 1, len(columns_names)):\n",
    "            # prepare data for the pair (Metric A, Metric B)\n",
    "            data = []\n",
    "            for corr, length in zip(correlations_list, lengths_list):\n",
    "                # only include valid correlations for analysis\n",
    "                if not corr.isna().iloc[i, j] and abs(corr.iloc[i, j]) != 1 and length > 3:\n",
    "                    data.append((corr.iloc[i, j], length))\n",
    "\n",
    "            # create a Polars DataFrame for the pair\n",
    "            df_data = pl.DataFrame(data, schema=['correlation', 'n'], orient='row')\n",
    "\n",
    "            if len(df_data) == 0:\n",
    "                continue\n",
    "\n",
    "            # calculate Q-statistic and test its significance\n",
    "            _, p_value_Q, is_significant = calculate_Q_and_test(df_data, alpha=alpha)\n",
    "\n",
    "            if is_significant:\n",
    "                significant_count += 1\n",
    "            else:\n",
    "                not_significant_count += 1\n",
    "\n",
    "    return {\n",
    "        'significant_count': significant_count,\n",
    "        'not_significant_count': not_significant_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_analysis_correlations(correlations_list, lengths_list, fixed=True):\n",
    "    \"\"\"\n",
    "    Performs a meta-analysis on correlations between all unique pairs of metrics\n",
    "    across multiple datasets.\n",
    "\n",
    "    Args:\n",
    "    - correlations_list (list of Polars DataFrame): List of correlation matrices.\n",
    "    - lengths_list (list of int): List of sample sizes corresponding to each correlation matrix.\n",
    "    - fixed (bool, optional): If True, performs a fixed-effects meta-analysis; otherwise, random-effects.\n",
    "\n",
    "    Returns:\n",
    "    - Polars DataFrame: Results of the meta-analysis for each pair of metrics, including:\n",
    "        - Metric A: Name of the first metric in the pair.\n",
    "        - Metric B: Name of the second metric in the pair.\n",
    "        - p-value: Statistical significance of the pooled correlation.\n",
    "        - effect: Pooled correlation effect size.\n",
    "        - interval_l: Lower bound of the confidence interval.\n",
    "        - interval_r: Upper bound of the confidence interval.\n",
    "        - significant: Whether the pooled correlation is significant (p-value <= alpha).\n",
    "    \"\"\"\n",
    "    columns_names = correlations_list[0].columns\n",
    "    results = []\n",
    "\n",
    "    # iterate over all unique pairs of columns (metrics)\n",
    "    for i in trange(len(columns_names)):\n",
    "        for j in range(i + 1, len(columns_names)):\n",
    "            # analyzing correlations between columns i and j\n",
    "            data = []\n",
    "            for corr, length in zip(correlations_list, lengths_list):\n",
    "                # include only valid correlation data points:\n",
    "                # - Not missing (not NaN).\n",
    "                # - Not equal to ±1 (perfect correlation).\n",
    "                # - Sample size greater than 3 (to avoid invalid variances).\n",
    "                if not corr.isna().iloc[i,j] and abs(corr.iloc[i, j]) != 1 and length > 3:\n",
    "                    data.append((corr.iloc[i, j], length))\n",
    "            # print(data)\n",
    "            df_data = pl.DataFrame(data, schema=['correlation', 'n'], orient='row')\n",
    "            if fixed:\n",
    "                result = fixed_effect(df_data)\n",
    "            else:\n",
    "                result = random_effects(df_data)\n",
    "            results.append((columns_names[i], columns_names[j], *result))\n",
    "\n",
    "    return pl.DataFrame(results, schema=['Metric A', 'Metric B', 'p-value', 'effect', 'interval_l', 'interval_r', 'significant'], orient='row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00, 11.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'significant_count': 300, 'not_significant_count': 0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_significant_Qs(correlations_df_list, lengths_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using random effects is more justified then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:03<00:00,  6.57it/s]\n"
     ]
    }
   ],
   "source": [
    "meta_analysis_fixed_results = meta_analysis_correlations(correlations_df_list, lengths_list, fixed=True)\n",
    "meta_analysis_fixed_results.write_csv(\"outputs/method-meta-analysis-fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_analysis_fixed_results.filter(\n",
    "    (pl.col('significant') == 1) & (pl.col('effect').abs() > 0.7)\n",
    ").sort(pl.col('effect').abs(), descending=True).write_csv('outputs/method-fixed-important.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00, 11.43it/s]\n"
     ]
    }
   ],
   "source": [
    "meta_analysis_random_results = meta_analysis_correlations(correlations_df_list, lengths_list, fixed=False)\n",
    "meta_analysis_random_results.write_csv(\"outputs/method-meta-analysis-random.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_analysis_random_results.filter(\n",
    "    (pl.col('significant') == 1) & (pl.col('effect').abs() > 0.7)\n",
    ").sort(pl.col('effect').abs(), descending=True).write_csv('outputs/method-random-important.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (8,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>p-value</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>0.0</td></tr><tr><td>3.5460e-289</td></tr><tr><td>6.9391e-271</td></tr><tr><td>1.9315e-224</td></tr><tr><td>2.8951e-168</td></tr><tr><td>1.7351e-127</td></tr><tr><td>1.5798e-25</td></tr><tr><td>4.2199e-24</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (8,)\n",
       "Series: 'p-value' [f64]\n",
       "[\n",
       "\t0.0\n",
       "\t3.5460e-289\n",
       "\t6.9391e-271\n",
       "\t1.9315e-224\n",
       "\t2.8951e-168\n",
       "\t1.7351e-127\n",
       "\t1.5798e-25\n",
       "\t4.2199e-24\n",
       "]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_analysis_fixed_results[\"p-value\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Metric A</th><th>Metric B</th><th>p-value</th><th>effect</th><th>interval_l</th><th>interval_r</th><th>significant</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;NUMPAR&quot;</td><td>&quot;Coupling&nbsp;Metric&nbsp;Rules&quot;</td><td>4.2199e-24</td><td>0.015867</td><td>0.012796</td><td>0.018937</td><td>1</td></tr><tr><td>&quot;McCC&quot;</td><td>&quot;TLOC&quot;</td><td>0.0</td><td>0.667346</td><td>0.665639</td><td>0.669046</td><td>1</td></tr><tr><td>&quot;TLOC&quot;</td><td>&quot;Anti&nbsp;Pattern&quot;</td><td>0.0</td><td>0.573065</td><td>0.570999</td><td>0.575124</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 7)\n",
       "┌──────────┬───────────────────────┬────────────┬──────────┬────────────┬────────────┬─────────────┐\n",
       "│ Metric A ┆ Metric B              ┆ p-value    ┆ effect   ┆ interval_l ┆ interval_r ┆ significant │\n",
       "│ ---      ┆ ---                   ┆ ---        ┆ ---      ┆ ---        ┆ ---        ┆ ---         │\n",
       "│ str      ┆ str                   ┆ f64        ┆ f64      ┆ f64        ┆ f64        ┆ i64         │\n",
       "╞══════════╪═══════════════════════╪════════════╪══════════╪════════════╪════════════╪═════════════╡\n",
       "│ NUMPAR   ┆ Coupling Metric Rules ┆ 4.2199e-24 ┆ 0.015867 ┆ 0.012796   ┆ 0.018937   ┆ 1           │\n",
       "│ McCC     ┆ TLOC                  ┆ 0.0        ┆ 0.667346 ┆ 0.665639   ┆ 0.669046   ┆ 1           │\n",
       "│ TLOC     ┆ Anti Pattern          ┆ 0.0        ┆ 0.573065 ┆ 0.570999   ┆ 0.575124   ┆ 1           │\n",
       "└──────────┴───────────────────────┴────────────┴──────────┴────────────┴────────────┴─────────────┘"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_analysis_fixed_results.sort(by='p-value', descending=True).gather_every(100).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (100, 2)\n",
      "┌─────────────┬──────┐\n",
      "│ correlation ┆ n    │\n",
      "│ ---         ┆ ---  │\n",
      "│ f64         ┆ i64  │\n",
      "╞═════════════╪══════╡\n",
      "│ 0.594921    ┆ 1573 │\n",
      "│ 0.698745    ┆ 7343 │\n",
      "│ 0.589581    ┆ 5560 │\n",
      "│ 0.382893    ┆ 5961 │\n",
      "│ 0.336824    ┆ 1052 │\n",
      "│ …           ┆ …    │\n",
      "│ 0.35063     ┆ 3375 │\n",
      "│ 0.696698    ┆ 4099 │\n",
      "│ 0.362922    ┆ 4965 │\n",
      "│ 0.44018     ┆ 2916 │\n",
      "│ 0.827112    ┆ 457  │\n",
      "└─────────────┴──────┘\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "j = -6\n",
    "# analyzing correlations between columns i and j\n",
    "data = []\n",
    "for corr, length in zip(correlations_df_list, lengths_list):\n",
    "    if not corr.isna().iloc[i,j] and abs(corr.iloc[i, j]) != 1 and length > 3:\n",
    "        data.append((corr.iloc[i, j], length))\n",
    "# print(data)\n",
    "df_data = pl.DataFrame(data, schema=['correlation', 'n'], orient='row')\n",
    "print(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.44587188267942246, 0.44340784677268497, 0.44832917944237216, True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = fixed_effect(df_data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to drop: ['HTRP', 'TCLOC', 'TNOS', 'HPL', 'LOC', 'HCPL', 'Size Metric Rules', 'HPV', 'TLLOC', 'HNDB', 'HVOL', 'TLOC']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_columns_to_drop(file_path, correlation_threshold=0.9):\n",
    "    df = pd.read_csv(file_path)\n",
    "    correlations = df[[\"Metric A\", \"Metric B\", \"effect\"]]\n",
    "    high_correlations = correlations[correlations[\"effect\"].abs() >= correlation_threshold]\n",
    "    columns_to_drop = set()\n",
    "    processed_columns = set()\n",
    "    \n",
    "    for _, row in high_correlations.iterrows():\n",
    "        col_a, col_b = row[\"Metric A\"], row[\"Metric B\"]\n",
    "        if col_a not in processed_columns and col_b not in processed_columns:\n",
    "            columns_to_drop.add(col_b)\n",
    "            processed_columns.add(col_a)\n",
    "            processed_columns.add(col_b)\n",
    "        elif col_a in processed_columns:\n",
    "            columns_to_drop.add(col_b)\n",
    "            processed_columns.add(col_b)\n",
    "        elif col_b in processed_columns:\n",
    "            columns_to_drop.add(col_a)\n",
    "            processed_columns.add(col_a)\n",
    "    \n",
    "    return list(columns_to_drop)\n",
    "\n",
    "file_path = \"outputs/method-random-important.csv\"\n",
    "columns_to_drop = find_columns_to_drop(file_path, correlation_threshold=0.9)\n",
    "print(\"Columns to drop:\", columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_cols = zero_col_list\n",
    "\n",
    "final_list_90 = empty_cols.copy()\n",
    "final_list_80 = empty_cols.copy()\n",
    "final_list_70 = empty_cols.copy()\n",
    "\n",
    "final_list_90.extend(find_columns_to_drop(file_path, 0.9))\n",
    "final_list_80.extend(find_columns_to_drop(file_path, 0.8))\n",
    "final_list_70.extend(find_columns_to_drop(file_path, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CD',\n",
       " 'NII',\n",
       " 'NL',\n",
       " 'NLE',\n",
       " 'TCD',\n",
       " 'WarningBlocker',\n",
       " 'WarningCritical',\n",
       " 'WarningMajor',\n",
       " 'WarningMinor',\n",
       " 'Documentation Metric Rules',\n",
       " 'HTRP',\n",
       " 'TCLOC',\n",
       " 'TNOS',\n",
       " 'HPL',\n",
       " 'LOC',\n",
       " 'HCPL',\n",
       " 'Size Metric Rules',\n",
       " 'HPV',\n",
       " 'TLLOC',\n",
       " 'HNDB',\n",
       " 'HVOL',\n",
       " 'TLOC']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CD',\n",
       " 'NII',\n",
       " 'NL',\n",
       " 'NLE',\n",
       " 'TCD',\n",
       " 'WarningBlocker',\n",
       " 'WarningCritical',\n",
       " 'WarningMajor',\n",
       " 'WarningMinor',\n",
       " 'Documentation Metric Rules',\n",
       " 'HTRP',\n",
       " 'TCLOC',\n",
       " 'TNOS',\n",
       " 'HPL',\n",
       " 'McCC',\n",
       " 'HEFF',\n",
       " 'LOC',\n",
       " 'HDIF',\n",
       " 'HCPL',\n",
       " 'Size Metric Rules',\n",
       " 'HPV',\n",
       " 'TLLOC',\n",
       " 'NOS',\n",
       " 'HNDB',\n",
       " 'HVOL',\n",
       " 'TLOC',\n",
       " 'DLOC']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list_80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CD',\n",
       " 'NII',\n",
       " 'NL',\n",
       " 'NLE',\n",
       " 'TCD',\n",
       " 'WarningBlocker',\n",
       " 'WarningCritical',\n",
       " 'WarningMajor',\n",
       " 'WarningMinor',\n",
       " 'Documentation Metric Rules',\n",
       " 'HTRP',\n",
       " 'TNOS',\n",
       " 'HPL',\n",
       " 'HEFF',\n",
       " 'Anti Pattern',\n",
       " 'TLLOC',\n",
       " 'McCC',\n",
       " 'HDIF',\n",
       " 'HPV',\n",
       " 'TLOC',\n",
       " 'LOC',\n",
       " 'Size Metric Rules',\n",
       " 'HVOL',\n",
       " 'DLOC',\n",
       " 'TCLOC',\n",
       " 'HCPL',\n",
       " 'WarningInfo',\n",
       " 'NOS',\n",
       " 'HNDB']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list_70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With threshold 0.9 we drop 22 columns, with threshold 0.8 we drop 27 columns, with threshold 0.7 we drop 29 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f\"With threshold 0.9 we drop {len(final_list_90)} columns, with threshold 0.8 we drop {len(final_list_80)} columns, with threshold 0.7 we drop {len(final_list_70)} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['McCC', 'HEFF', 'HDIF', 'NOS', 'DLOC']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in final_list_80 if x not in final_list_90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anti Pattern', 'WarningInfo']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in final_list_70 if x not in final_list_80]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
